{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b209f18-98bc-4fa3-9916-41aa0fd38dd4",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This hands‑on tutorial begins with a quick baseline RAG prototype for Roman‑Empire question‑answering and uses it as a **stepping stone toward the real objective: running a fully local Retrieval‑Augmented Generation system powered by a fine‑tuned small language model (SLM) that you train with the distil labs platform and then serve on your own hardware.**\n",
    "\n",
    "* **Chapter 1 – Build a local RAG system**\n",
    "\n",
    "  * Launch a lightweight chat model locally with **vLLM**.\n",
    "  * Chunk a Wikipedia article, embed the chunks with **HuggingFace sentence‑transformers**, and store them in an **in‑memory vector store**.\n",
    "  * Glue retrieval and generation together in a minimal **RAG class**, then test the loop end‑to‑end.\n",
    "\n",
    "* **Chapter 2 – Specialize an SLM for local RAG**\n",
    "\n",
    "  * Collect a *tiny* labelled QA set plus the unstructured passages you just indexed.\n",
    "  * Upload them to **distil labs**, evaluate a powerful teacher LLM, and fine‑tune a 135 M‑parameter student model.\n",
    "\n",
    "* **Chapter 3 – Rerun the RAG loop with the fine‑tuned model**\n",
    "\n",
    "  * Serve the freshly trained weights locally with vLLM.\n",
    "  * Drop the tuned model into the same RAG class and observe the quality gains.\n",
    "\n",
    "By the end you will have a fully working RAG system that you can run on a laptop, plus a recipe for squeezing extra accuracy out of modest hardware by distilling knowledge from a large model into a small one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c79f7c-d3b3-45c9-ba2c-96c91189b978",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c8c64-0d9b-4315-9c95-c6df91585c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "\n",
    "pip install vllm==0.7.3\n",
    "pip install langchain-core langchain_community langchain-openai langchain-huggingface\n",
    "pip install wikipedia pandas numpy requests rich pyyaml rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5002d3-f8e4-437e-ada9-e2d8013a8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d1449-3ccb-472a-b715-d8981c830c39",
   "metadata": {},
   "source": [
    "# Chapter 1: Build a local RAG system\n",
    "\n",
    "In this first chapter **we will build** a baseline RAG prototype that runs entirely on your machine: we will serve a small chat model with vLLM, index the Roman Empire Wikipedia article, and glue retrieval to generation with a concise helper class. This will give us a reference point whose quality we’ll aim to surpass after fine‑tuning in Chapter 2.\n",
    "\n",
    "### Load our local model\n",
    "\n",
    "Start the inference server with the chat model. If the base command does not work for you, you can also try:\n",
    "\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model HuggingFaceTB/SmolLM2-135M-Instruct \\\n",
    "    --device cpu \\\n",
    "    --port 8000 2>&1 | tee chat-model.log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030762b-502d-4014-a4d0-8ccca1daa9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg\n",
    "\n",
    "vllm serve HuggingFaceTB/SmolLM2-135M-Instruct --device cpu --port 8000 2>&1 | tee chat-model.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2fdde-b0b1-4cd3-b10d-e9f0bff8d30a",
   "metadata": {},
   "source": [
    "Make requests against our server using a standard Langchain interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c17e2-f82d-4806-9179-a94839724bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    model=\"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    ")\n",
    "\n",
    "response = llm.invoke([\n",
    "    {\"role\": \"user\", \"content\": \"What is today's date?\"}\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c62a4-b33c-45f8-b8f0-6499f789c76b",
   "metadata": {},
   "source": [
    "To kill the local server, run the following snippet in a new cell:\n",
    "\n",
    "```bash\n",
    "! pgrep -al -f \"vllm serve\"\n",
    "! pkill -al -f \"vllm serve\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f09d7-ba31-4ea6-bf31-64e6eefdf47a",
   "metadata": {},
   "source": [
    "### Index our target dataset\n",
    "\n",
    "This section walks through loading the **Wikipedia article on the Roman Empire** into an in‑memory vector store (adapted from [https://python.langchain.com/docs/tutorials/rag/](https://python.langchain.com/docs/tutorials/rag/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37b8d5-f988-4000-8bbc-b4eb63541d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Load and chunk the Wikipedia page\n",
    "loader = WikipediaLoader(\n",
    "    query=\"Roman Empire\",\n",
    "    lang=\"en\",\n",
    "    load_max_docs=1,\n",
    "    load_all_available_meta=False,\n",
    "    doc_content_chars_max=int(1e7)\n",
    ")\n",
    "docs = loader.load()\n",
    "docs[0].metadata.pop(\"summary\")  # Remove the long summary\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and index the chunks\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "indexed = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf4e28-4d9f-4410-adaa-1cf1eed8d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efd0f6-9780-4de0-8316-b7d32c5fe653",
   "metadata": {},
   "source": [
    "### Define the RAG logic\n",
    "\n",
    "Now that our dataset is indexed and the chat model is live, we can **wire retrieval and generation together**. In this section we implement a bite‑sized `RAG` helper class that\n",
    "\n",
    "1. fetches the top‑k passages most similar to the user’s question,\n",
    "2. feeds those passages and the question into the language model via a structured prompt, and\n",
    "3. returns a concise answer.\n",
    "\n",
    "With this plumbing in place, answering a question becomes a single‑function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a6c2a-aca4-4b95-9296-aba5888ca5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, vector_store: InMemoryVectorStore, llm: ChatOpenAI):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm\n",
    "\n",
    "        self.SYSTEM_PROMPT = \"\"\"\n",
    "You are a problem solving model working on task_description XML block:\n",
    "\n",
    "<task_description>\n",
    "Answer the question using information from one of the context paragrpahs.\n",
    "The answer should should contain all important information from the context paragraph but stay short, one sentence maximum.\n",
    "</task_description>\n",
    "\n",
    "You will be given a single task with context in the context XML block and the task in the question XML block\n",
    "Solve the task in question block based on the context in context block.\n",
    "Generate only the answer, do not generate anything else\n",
    "\"\"\"\n",
    "\n",
    "        self.PROMPT_TEMPLATE = \"\"\"\n",
    "Now for the real task, solve the task in question block based on the context in context block.\n",
    "Generate only the solution, do not generate anything else.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\"\n",
    "\n",
    "    def retrieve(self, question: str, k: int = 3):\n",
    "        return self.vector_store.similarity_search(question, k=k)\n",
    "\n",
    "    def generate(self, question: str, context_docs):\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in context_docs)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": self.PROMPT_TEMPLATE.format(context=context, question=question)},\n",
    "        ]\n",
    "        return self.llm.invoke(messages).content\n",
    "\n",
    "    def answer(self, question: str):\n",
    "        return self.generate(question, self.retrieve(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b6fda-3b61-44be-bf15-2013fb6e3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(vector_store=vector_store, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242f6bf-d488-4dfb-b58f-a928ad770322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag.answer(\"When did the Roman Empire collapse?\"))\n",
    "# print(rag.answer(\"What were the main metals mined in the Iberian Peninsula during the Roman Empire?\"))\n",
    "# print(rag.answer(\"How did the seating arrangement in Roman amphitheatres reflect social hierarchy?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b19a4f-59fe-4fd5-bd90-bc524b2254f7",
   "metadata": {},
   "source": [
    "### Test our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a381a7-3d83-4d67-9b9a-bd3605ee2b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "results = []\n",
    "for idx, example in test_dataset.iterrows():\n",
    "    reference = example[\"answer\"]\n",
    "    prediction = rag.answer(example[\"question\"])\n",
    "    metric = rouge.score(\n",
    "        target=reference,\n",
    "        prediction=prediction,\n",
    "    )\n",
    "    results.append(metric[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(f\"Aggregate metric: {np.mean(results)} +/- {np.std(results)}\")\n",
    "# Aggregate metric: 0.1618807343679535 +/- 0.10965436802120945"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6dc736-7f5b-4578-8a64-b8f5a6a588ed",
   "metadata": {},
   "source": [
    "# Chapter 2: Specialize an SLM for local RAG\n",
    "\n",
    "In this chapter we will fine‑tune a small language model with **distil labs** in three steps:\n",
    "\n",
    "1. **Data upload** – upload the job description, train/test splits, unstructured contexts, and a YAML config.\n",
    "2. **Teacher evaluation** – verify that a large model can solve the task; its accuracy becomes the benchmark.\n",
    "3. **SLM training** – distil the teacher’s behaviour into a 135 M‑parameter student."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b352dc-540b-469c-b844-433469f1137d",
   "metadata": {},
   "source": [
    "### Authentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c83a4-763e-4b8a-8bae-274b19de5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def distil_bearer_token(DL_USERNAME: str, DL_PASSWORD: str) -> str:\n",
    "    response = requests.post(\n",
    "        \"https://cognito-idp.eu-central-1.amazonaws.com\",\n",
    "        headers={\n",
    "            \"X-Amz-Target\": \"AWSCognitoIdentityProviderService.InitiateAuth\",\n",
    "            \"Content-Type\": \"application/x-amz-json-1.1\",\n",
    "        },\n",
    "        data=json.dumps({\n",
    "            \"AuthParameters\": {\n",
    "                \"USERNAME\": DL_USERNAME,\n",
    "                \"PASSWORD\": DL_PASSWORD,\n",
    "            },\n",
    "            \"AuthFlow\": \"USER_PASSWORD_AUTH\",\n",
    "            \"ClientId\" : \"4569nvlkn8dm0iedo54nbta6fd\",\n",
    "        })\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"AuthenticationResult\"][\"AccessToken\"]\n",
    "\n",
    "\n",
    "DL_USERNAME=\"DL_USERNAME\"\n",
    "DL_PASSWORD=\"DL_PASSWORD\"\n",
    "\n",
    "AUTH_HEADER = {\"Authorization\": distil_bearer_token(DL_USERNAME, DL_PASSWORD)}\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac10fcc-981e-42b6-bb7e-dee5312d6197",
   "metadata": {},
   "source": [
    "### Data Upload\n",
    "\n",
    "The data for this example should be stored in the data_location directory. Lets first take a look at the current directory to make sure all files are available. Your current directory should look like:\n",
    "```\n",
    "├── README.md\n",
    "├── rag-tutorial.ipynb\n",
    "└── data\n",
    "    ├── job_description.json\n",
    "    ├── test.csv\n",
    "    ├── train.csv\n",
    "    └── unstructured.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bd089-b9ed-4e4b-9626-d93628b56bb8",
   "metadata": {},
   "source": [
    "#### Job Description\n",
    "A job description explains the question answering task in plain english and follows the general structure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ac7ea-74a9-4006-9d2c-d46472afd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import rich.json\n",
    "\n",
    "with open(Path(\"data\").joinpath(\"job_description.json\")) as fin:\n",
    "    rich.print(rich.json.JSON(fin.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a65edf-29b5-4d0c-97b0-cc5776af0967",
   "metadata": {},
   "source": [
    "#### Train/test set\n",
    "\n",
    "We need a small train dataset to begin distil labs training and a testing dataset that we can use to evaluate the performance of the fine-tuned model. Here, we use the train and test datasets from the data_location directory where each is a CSV file with below 100 (question, answer) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11eb6be-08ba-4d8f-858d-5d05d2ba2f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"# --- Train set\")\n",
    "train = pd.read_csv(Path(\"data\").joinpath(\"train.csv\"))\n",
    "display(train)\n",
    "\n",
    "print(\"# --- Test set\")\n",
    "test = pd.read_csv(Path(\"data\").joinpath(\"test.csv\"))\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f672f7-fead-4616-8c38-a6bf45efdc1f",
   "metadata": {},
   "source": [
    "#### Unstructured dataset\n",
    "The unstructured dataset is used to guide the teacher model in generating diverse, domain-specific data. Here, we will use the indexed contexts to create the unstructured dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a33ecd-ddfc-4284-a34e-60dbe3d85e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "contexts_dataframe = pd.DataFrame([split.model_dump() for split in all_splits])\n",
    "contexts_dataframe = contexts_dataframe.rename(columns={\"page_content\": \"context\"})[[\"context\"]]\n",
    "contexts_dataframe.to_csv(\"data/contexts.jsonl\")\n",
    "contexts_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4d1ad-2c87-4914-a3e0-28947a0209e6",
   "metadata": {},
   "source": [
    "#### Data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a8242-3506-40b1-8aa3-098b4b0f4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Specify the config\n",
    "config = {\n",
    "    \"base\": {\"task\": \"question-answering-open-book\"},\n",
    "    \"setup\": {\"student_model_name\": \"HuggingFaceTB/SmolLM2-135M-Instruct\"},\n",
    "    \"synthgen\": {\n",
    "        \"data_generation_strategy\": \"qa-open-book\",\n",
    "    },\n",
    "    \"tuning\": {\n",
    "        \"num_train_epochs\": 8,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Package your data\n",
    "data_dir = Path(\"data\")\n",
    "data = {\n",
    "    \"job_description.json\": open(data_dir / \"job_description.json\", encoding=\"utf-8\").read(),\n",
    "    \"train.csv\": open(data_dir / \"train.csv\", encoding=\"utf-8\").read(),\n",
    "    \"test.csv\": open(data_dir / \"test.csv\", encoding=\"utf-8\").read(),\n",
    "    \"unstructured.csv\": open(data_dir / \"unstructured.csv\", encoding=\"utf-8\").read(),\n",
    "    \"config.yaml\": yaml.dump(config),\n",
    "}\n",
    "\n",
    "# Upload data\n",
    "response = requests.post(\n",
    "    \"https://api.distillabs.ai/uploads\",\n",
    "    data=json.dumps(data),\n",
    "    headers={\"content-type\": \"application/json\", **AUTH_HEADER},\n",
    ")\n",
    "upload_id = response.json().get(\"id\")\n",
    "print(f\"Upload successful. ID: {upload_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8cf6b2-b402-4ea3-8bce-f88e0378f1c5",
   "metadata": {},
   "source": [
    "### Teacher Evaluation\n",
    "Before training an SLM, distil labs validates whether a large language model can solve your task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd2692-f262-4aa6-a698-757428ac6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Start teacher evaluation\n",
    "response = requests.post(\n",
    "    f\"https://api.distillabs.ai/teacher-evaluations/{upload_id}\",\n",
    "    headers=AUTH_HEADER\n",
    ")\n",
    "\n",
    "teacher_evaluation_id = response.json().get(\"id\")\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f99f6-bb40-4d64-a23a-08e21bbafcf6",
   "metadata": {},
   "source": [
    "Poll the status endpoint until it completes, then inspect the accuracy. If accuracy is unsatisfactory, refine the job description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110da813-d290-4f6c-bdb0-46d75dd3f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    f\"https://api.distillabs.ai/teacher-evaluations/{teacher_evaluation_id}/status\",\n",
    "    headers=AUTH_HEADER\n",
    ")\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23394ebe-1dc5-4639-8e98-b8847aeb8ece",
   "metadata": {},
   "source": [
    "### SLM Training\n",
    "Once the teacher evaluation completes successfully, start the SLM training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845a128-1f49-48e5-9f03-e291a476447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "# Start SLM training\n",
    "response = requests.post(\n",
    "    f\"https://api.distillabs.ai/trainings/{upload_id}\",\n",
    "    headers=AUTH_HEADER,\n",
    ")\n",
    "\n",
    "pprint(response.json())\n",
    "slm_training_job_id = response.json().get(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981fa42-2d06-4a47-81b1-41a5a19d87f8",
   "metadata": {},
   "source": [
    "We can analyze the status of the training job using the `jobs` API. The following code snippets displays the current status of the job we started before. When the job is finished (`status=complete`), we can use the `jobs` API again to get the benchmarking result - the accuracy of the LLM and the accuracy of the fine-tuned SLM. We can achieve this using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124daab6-2fb9-4907-b6da-38d22bf20c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "response = requests.get(\n",
    "    f\"https://api.distillabs.ai/trainings/{slm_training_job_id}/status\",\n",
    "    headers=AUTH_HEADER,\n",
    ")\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7223fc-bad9-463e-9f27-8c7bd508a001",
   "metadata": {},
   "source": [
    "When the job is finished (`status=complete`), we can use the `jobs` API again to get the benchmarking result - the accuracy of the LLM and the accuracy of the fine-tuned SLM. We can achieve this using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e887a11-66b5-4ff9-afb3-bf9ceb396a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "response = requests.get(\n",
    "    f\"https://api.distillabs.ai/trainings/{slm_training_job_id}/evaluation-results\",\n",
    "    headers=AUTH_HEADER,\n",
    ")\n",
    "\n",
    "pprint(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1c8e9-75d1-4274-9136-979bad9bde91",
   "metadata": {},
   "source": [
    "### Download Your Model\n",
    "Once training is complete, download your model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbffcd-2e1e-47b4-9021-32687fe22a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model download URL\n",
    "response = requests.get(\n",
    "    f\"https://api.distillabs.ai/trainings/{slm_training_job_id}/model\",\n",
    "    headers=AUTH_HEADER\n",
    ")\n",
    "\n",
    "pprint(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3eee98-0fce-4129-9a10-22f82767a0e5",
   "metadata": {},
   "source": [
    "# Chapter 3: Build a local RAG system with the fine‑tuned model\n",
    "\n",
    "Now that we have a small language model fine‑tuned specifically for Roman‑Empire question‑answering, we can rebuild our RAG pipeline around it. This domain‑specialized LLM will provide more accurate, context‑aware answers than our baseline model while still running entirely on local hardware. In this chapter we’ll serve the tuned weights with vLLM, swap the model into our RAG helper class, and validate the performance gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651e034-ce4c-4d70-a557-91924203c092",
   "metadata": {},
   "source": [
    "### Serve a local fine-tuned LLM\n",
    "Kill all previous instances of vllm serving and start a new model. We need to direct the vllm server at the location of the new model which will be different depending on your settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d490c-7568-4482-a72f-274b3999fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pgrep -al -f \"vllm serve\"\n",
    "! pkill -al -f \"vllm serve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf79b9-50f1-4854-8326-3f589189d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg\n",
    "\n",
    "vllm serve <YOUR_LOCATION> --device cpu --port 8000 2>&1 | tee chat-tuned-model.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f4599-26f2-48f4-9028-b7ae7831d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import pprint\n",
    "\n",
    "tuned_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    model=\"<YOUR_LOCATION>\")\n",
    "\n",
    "response = tuned_llm.invoke([{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"When did the roman empire fall?\"\n",
    "}])\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca350f-909a-45a3-89b5-da3e75488d9d",
   "metadata": {},
   "source": [
    "### Plug the new model into RAG\n",
    "With the fine‑tuned weights now running locally, the last step is to swap the specialized LLM into our existing RAG helper class. The retrieval component remains identical—still fetching the most relevant passages about the Roman Empire—while the generation step now leverages a model that has been trained on our domain‑specific data. This simple drop‑in replacement should yield noticeably sharper and more accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87c130-d6d6-41f8-a1a9-1a7a019a5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rag = RAG(vector_store=vector_store, llm=tuned_llm)\n",
    "print(tuned_rag.answer(\"When did the roman empire collapse?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33653e93-e07a-4852-8fb1-2451ca6b045c",
   "metadata": {},
   "source": [
    "### Test our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d75b0-5ffb-4393-82eb-0852ebb8b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "results = []\n",
    "for idx, example in test_dataset.iterrows():\n",
    "    reference = example[\"answer\"]\n",
    "    prediction = tuned_rag.answer(example[\"question\"])\n",
    "    metric = rouge.score(\n",
    "        target=reference,\n",
    "        prediction=prediction,\n",
    "    )\n",
    "    results.append(metric[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(f\"Aggregate metric: {np.mean(results)} +/- {np.std(results)}\")\n",
    "# Aggregate metric: 0.253503946795671 +/- 0.16689695917279943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652bff2-3bf1-4ae4-b99d-cf98aba831bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
