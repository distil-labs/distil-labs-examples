{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce420c73",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the **Distil Labs** hands‑on tutorial.  \n",
    "In this walkthrough you will:\n",
    "\n",
    "1. **Specialize** a small language model (SLM) with Distil Labs.\n",
    "2. **Build** a fully local retrieval‑augmented‑generation (RAG) pipeline that uses your fine‑tuned model.\n",
    "\n",
    "By the end you’ll have a domain‑aware assistant you can run entirely on your own hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c79f7c-d3b3-45c9-ba2c-96c91189b978",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bb833-7a5d-4cd4-859e-c0cd0ee0184b",
   "metadata": {},
   "source": [
    "##### Install ollama\n",
    "\n",
    "To install ollama, follow the instructions in https://ollama.com/download. If you are on a linux platform the following command should get the job done but otherwise, the download page should cover the installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816c3d5-1fea-4211-b41e-1f7f814f821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946db5d-60cb-479c-adec-fabe603a3622",
   "metadata": {},
   "source": [
    "Once ollama is installed, we should start the application. If you installed ollama using the installer, make sure the app is running. If you installed ollama with with CLI, you can start the deamon with `ollama serve`, for example using `nohup` to make sure it stays in the bacgkround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1dee8-3a2b-4355-8375-f0e578eca36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nohup ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601815c4-67f3-4b46-bb0b-3ab4768b3cdf",
   "metadata": {},
   "source": [
    "Make sure the app is running by executing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b8f32-4c0b-440b-b2a1-39673465ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d92f5-d762-4357-bc45-f5ea0f75c8f8",
   "metadata": {},
   "source": [
    "##### Install python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae4070-9e0a-49f5-818a-8a520b5b933d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install langchain-core langchain_community langchain-openai langchain-huggingface langchain-ollama\n",
    "! pip install wikipedia pandas numpy requests rich pyyaml rouge_score ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7dac9-f8ee-4a07-a545-4bf698315c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791919fe",
   "metadata": {},
   "source": [
    "# Step 1: Understand your data\n",
    "\n",
    "Before we can specialize a model or build a retrieval‑augmented generation (RAG) pipeline, we need to **inspect the knowledge source** we’ll be working with. In this tutorial, our task is: **answer questions about the Roman Empire**.\n",
    "\n",
    "_Why bother looking at the raw data first?_  \n",
    "• It clarifies the scope (what’s _in_ and what’s _out_ of domain).  \n",
    "• It helps us spot formatting issues or noisy sections.  \n",
    "• It lets us craft realistic evaluation questions early on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded1c5b",
   "metadata": {},
   "source": [
    "### Retrieve a reference article\n",
    "To keep things quick, we’ll use a single reference document: the English **Wikipedia** page for the Roman Empire.\n",
    "In a production system you’d likely combine multiple sources, but one page is enough to demo the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia, textwrap\n",
    "\n",
    "# Disable auto-suggest so we get the exact page\n",
    "page = wikipedia.page(\"Roman Empire\", auto_suggest=False)\n",
    "wikipedia_text = page.content\n",
    "\n",
    "print(\"First 1200 characters of the article:\\n\")\n",
    "print(textwrap.fill(wikipedia_text[:1200], 110))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f7755",
   "metadata": {},
   "source": [
    "### Sample questions we want to answer\n",
    "Let’s jot down a few questions that our finished system should handle. Capturing these **early** gives us a mini test‑set for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd329f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qa = [\n",
    "    {\"question\": \"Who was the first emperor of the Roman Empire?\", \"answer\": \"Augustus (formerly Octavian)\"},\n",
    "    {\"question\": \"When did the Roman Empire collapse?\", \"answer\": \"The west Roman Empire fell in 476 CE and the Easy laster until the fall of Constantinopol in 1453.\"},\n",
    "    {\"question\": \"Which two languages were most widely spoken across the empire?\", \"answer\": \"Latin in the West and Greek in the East\"},\n",
    "    {\"question\": \"What monumental arena in Rome hosted gladiatorial games?\", \"answer\": \"The Colosseum\"},\n",
    "]\n",
    "\n",
    "for qa in sample_qa:\n",
    "    print(f\"Q: {qa['question']}\\nA: {qa['answer']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6dc736-7f5b-4578-8a64-b8f5a6a588ed",
   "metadata": {},
   "source": [
    "# Step 2: Specialize a question answering model with Distil Labs\n",
    "\n",
    "In this chapter we will fine‑tune a small language model with **distil labs** in three steps:\n",
    "\n",
    "1. **Data upload** – upload the job description, train/test splits, unstructured contexts, and a YAML config.\n",
    "2. **Teacher evaluation** – verify that a large model can solve the task; its accuracy becomes the benchmark.\n",
    "3. **SLM training** – distil the teacher’s behaviour into a 135 M‑parameter student."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b352dc-540b-469c-b844-433469f1137d",
   "metadata": {},
   "source": [
    "### Authentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c83a4-763e-4b8a-8bae-274b19de5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def distil_bearer_token(DL_USERNAME: str, DL_PASSWORD: str) -> str:\n",
    "    response = requests.post(\n",
    "        \"https://cognito-idp.eu-central-1.amazonaws.com\",\n",
    "        headers={\n",
    "            \"X-Amz-Target\": \"AWSCognitoIdentityProviderService.InitiateAuth\",\n",
    "            \"Content-Type\": \"application/x-amz-json-1.1\",\n",
    "        },\n",
    "        data=json.dumps({\n",
    "            \"AuthParameters\": {\n",
    "                \"USERNAME\": DL_USERNAME,\n",
    "                \"PASSWORD\": DL_PASSWORD,\n",
    "            },\n",
    "            \"AuthFlow\": \"USER_PASSWORD_AUTH\",\n",
    "            \"ClientId\" : \"4569nvlkn8dm0iedo54nbta6fd\",\n",
    "        })\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"AuthenticationResult\"][\"AccessToken\"]\n",
    "\n",
    "\n",
    "DL_USERNAME=\"DL_USERNAME\"\n",
    "DL_PASSWORD=\"DL_PASSWORD\"\n",
    "\n",
    "AUTH_HEADER = {\"Authorization\": distil_bearer_token(DL_USERNAME, DL_PASSWORD)}\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac10fcc-981e-42b6-bb7e-dee5312d6197",
   "metadata": {},
   "source": [
    "### Data Upload\n",
    "\n",
    "The data for this example should be stored in the data_location directory. Lets first take a look at the current directory to make sure all files are available. Your current directory should look like:\n",
    "```\n",
    "├── README.md\n",
    "├── rag-tutorial.ipynb\n",
    "└── data\n",
    "    ├── job_description.json\n",
    "    ├── test.csv\n",
    "    ├── train.csv\n",
    "    └── unstructured.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bd089-b9ed-4e4b-9626-d93628b56bb8",
   "metadata": {},
   "source": [
    "#### Job Description\n",
    "A job description explains the question answering task in plain english and follows the general structure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ac7ea-74a9-4006-9d2c-d46472afd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import rich.json\n",
    "\n",
    "with open(Path(\"data\").joinpath(\"job_description.json\")) as fin:\n",
    "    rich.print(rich.json.JSON(fin.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a65edf-29b5-4d0c-97b0-cc5776af0967",
   "metadata": {},
   "source": [
    "#### Train/test set\n",
    "\n",
    "We need a small train dataset to begin distil labs training and a testing dataset that we can use to evaluate the performance of the fine-tuned model. Here, we use the train and test datasets from the data_location directory where each is a CSV file with below 100 (question, answer) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11eb6be-08ba-4d8f-858d-5d05d2ba2f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"# --- Train set\")\n",
    "train = pd.read_csv(Path(\"data\").joinpath(\"train.csv\"))\n",
    "display(train)\n",
    "\n",
    "print(\"# --- Test set\")\n",
    "test = pd.read_csv(Path(\"data\").joinpath(\"test.csv\"))\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f672f7-fead-4616-8c38-a6bf45efdc1f",
   "metadata": {},
   "source": [
    "#### Unstructured dataset\n",
    "The unstructured dataset is used to guide the teacher model in generating diverse, domain-specific data. Here, we will use the chunks of the wikipedia article as the unstructured data for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a33ecd-ddfc-4284-a34e-60dbe3d85e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_text(wikipedia_text)\n",
    "\n",
    "# Save the documents into local storage\n",
    "contexts_dataframe = pd.DataFrame([{\"context\": split} for split in all_splits])\n",
    "contexts_dataframe.to_csv(\"data/unstructured.csv\")\n",
    "contexts_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4d1ad-2c87-4914-a3e0-28947a0209e6",
   "metadata": {},
   "source": [
    "#### Data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a8242-3506-40b1-8aa3-098b4b0f4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Specify the config\n",
    "config = {\n",
    "    \"base\": {\"task\": \"question-answering-open-book\"},\n",
    "    \"setup\": {\"student_model_name\": \"HuggingFaceTB/SmolLM2-135M-Instruct\"},\n",
    "    \"synthgen\": {\n",
    "        \"data_generation_strategy\": \"qa-open-book\",\n",
    "    },\n",
    "    \"tuning\": {\n",
    "        \"num_train_epochs\": 8,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Package your data\n",
    "data_dir = Path(\"data\")\n",
    "data = {\n",
    "    \"job_description.json\": open(data_dir / \"job_description.json\", encoding=\"utf-8\").read(),\n",
    "    \"train.csv\": open(data_dir / \"train.csv\", encoding=\"utf-8\").read(),\n",
    "    \"test.csv\": open(data_dir / \"test.csv\", encoding=\"utf-8\").read(),\n",
    "    \"unstructured.csv\": open(data_dir / \"unstructured.csv\", encoding=\"utf-8\").read(),\n",
    "    \"config.yaml\": yaml.dump(config),\n",
    "}\n",
    "\n",
    "# Upload data\n",
    "response = requests.post(\n",
    "    \"https://api.distillabs.ai/uploads\",\n",
    "    data=json.dumps(data),\n",
    "    headers={\"content-type\": \"application/json\", **AUTH_HEADER},\n",
    ")\n",
    "upload_id = response.json().get(\"id\")\n",
    "print(f\"Upload successful. ID: {upload_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8cf6b2-b402-4ea3-8bce-f88e0378f1c5",
   "metadata": {},
   "source": [
    "### Teacher Evaluation\n",
    "Before training an SLM, distil labs validates whether a large language model can solve your task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd2692-f262-4aa6-a698-757428ac6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Start teacher evaluation\n",
    "response = requests.post(\n",
    "    f\"https://api.distillabs.ai/teacher-evaluations/{upload_id}\",\n",
    "    headers=AUTH_HEADER\n",
    ")\n",
    "\n",
    "teacher_evaluation_id = response.json().get(\"id\")\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f99f6-bb40-4d64-a23a-08e21bbafcf6",
   "metadata": {},
   "source": [
    "Poll the status endpoint until it completes, then inspect the accuracy. If accuracy is unsatisfactory, refine the job description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110da813-d290-4f6c-bdb0-46d75dd3f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    f\"https://api.distillabs.ai/teacher-evaluations/{teacher_evaluation_id}/status\",\n",
    "    headers=AUTH_HEADER\n",
    ")\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23394ebe-1dc5-4639-8e98-b8847aeb8ece",
   "metadata": {},
   "source": [
    "### SLM Training\n",
    "Once the teacher evaluation completes successfully, start the SLM training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845a128-1f49-48e5-9f03-e291a476447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "# Start SLM training\n",
    "response = requests.post(\n",
    "    f\"https://api.distillabs.ai/trainings/{upload_id}\",\n",
    "    headers=AUTH_HEADER,\n",
    ")\n",
    "\n",
    "pprint(response.json())\n",
    "slm_training_job_id = response.json().get(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981fa42-2d06-4a47-81b1-41a5a19d87f8",
   "metadata": {},
   "source": [
    "We can analyze the status of the training job using the `jobs` API. The following code snippets displays the current status of the job we started before. When the job is finished (`status=complete`), we can use the `jobs` API again to get the benchmarking result - the accuracy of the LLM and the accuracy of the fine-tuned SLM. We can achieve this using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124daab6-2fb9-4907-b6da-38d22bf20c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "response = requests.get(\n",
    "    f\"https://api.distillabs.ai/trainings/{slm_training_job_id}/status\",\n",
    "    headers=AUTH_HEADER,\n",
    ")\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7223fc-bad9-463e-9f27-8c7bd508a001",
   "metadata": {},
   "source": [
    "When the job is finished (`status=complete`), we can use the `jobs` API again to get the benchmarking result - the accuracy of the LLM and the accuracy of the fine-tuned SLM. We can achieve this using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e887a11-66b5-4ff9-afb3-bf9ceb396a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "response = requests.get(\n",
    "    f\"https://api.distillabs.ai/trainings/{slm_training_job_id}/evaluation-results\",\n",
    "    headers=AUTH_HEADER,\n",
    ")\n",
    "\n",
    "pprint(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1c8e9-75d1-4274-9136-979bad9bde91",
   "metadata": {},
   "source": [
    "### Download Your Model\n",
    "Once training is complete, download your model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbffcd-2e1e-47b4-9021-32687fe22a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model download URL\n",
    "response = requests.get(\n",
    "    f\"https://api.distillabs.ai/trainings/{slm_training_job_id}/model\",\n",
    "    headers=AUTH_HEADER\n",
    ")\n",
    "\n",
    "pprint(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3eee98-0fce-4129-9a10-22f82767a0e5",
   "metadata": {},
   "source": [
    "# Step 3: Build a local RAG system with your fine‑tuned model\n",
    "\n",
    "Now that we have a small language model fine‑tuned specifically for Roman‑Empire question‑answering, we can build our RAG pipeline around it. This domain‑specialized LLM will provide more accurate, context‑aware answers than our baseline model while still running entirely on local hardware. The main objectives for us are as follows:\n",
    "  * Launch a lightweight chat model locally with **ollama**.\n",
    "  * Chunk a Wikipedia article, embed the chunks with **HuggingFace sentence‑transformers**, and store them in an **in‑memory vector store**.\n",
    "  * Glue retrieval and generation together in a minimal **RAG class**, then test the loop end‑to‑end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf599e-4811-46e1-905f-cb176e3b62f8",
   "metadata": {},
   "source": [
    "### Register and test the downloaded model \n",
    "Once your model is trained, it should be unpacked and registered with ollama. The downloaded model directory already contains everything that is needed and the model can be registed with the command below. Once it is ready, we can test the model with a standard OPenAI interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f142dc-15d4-4a20-8953-494b4883b577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ollama create model-distillabs -f model/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6631a9-6389-41e4-8515-4bc7be43ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"model-distillabs\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"When did the roman empire collapse?\"},\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f09d7-ba31-4ea6-bf31-64e6eefdf47a",
   "metadata": {},
   "source": [
    "### Index our target dataset\n",
    "\n",
    "This section walks through loading the **Wikipedia article on the Roman Empire** into an in‑memory vector store (adapted from [https://python.langchain.com/docs/tutorials/rag/](https://python.langchain.com/docs/tutorials/rag/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37b8d5-f988-4000-8bbc-b4eb63541d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_splits = text_splitter.split_text(wikipedia_text)\n",
    "document_splits = text_splitter.create_documents(text_splits)\n",
    "\n",
    "# Embed and index the chunks\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "indexed = vector_store.add_documents(documents=document_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efd0f6-9780-4de0-8316-b7d32c5fe653",
   "metadata": {},
   "source": [
    "### Define the RAG logic\n",
    "\n",
    "Now that our dataset is indexed and the chat model is live, we can **wire retrieval and generation together**. In this section we implement a bite‑sized `RAG` helper class that\n",
    "\n",
    "1. fetches the top‑k passages most similar to the user’s question,\n",
    "2. feeds those passages and the question into the language model via a structured prompt, and\n",
    "3. returns a concise answer.\n",
    "\n",
    "With this plumbing in place, answering a question becomes a single‑function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a6c2a-aca4-4b95-9296-aba5888ca5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, vector_store: InMemoryVectorStore, llm: ChatOpenAI):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm\n",
    "\n",
    "        self.SYSTEM_PROMPT = \"\"\"\n",
    "You are a problem solving model working on task_description XML block:\n",
    "\n",
    "<task_description>\n",
    "Answer the question using information from one of the context paragrpahs.\n",
    "The answer should should contain all important information from the context paragraph but stay short, one sentence maximum.\n",
    "</task_description>\n",
    "\n",
    "You will be given a single task with context in the context XML block and the task in the question XML block\n",
    "Solve the task in question block based on the context in context block.\n",
    "Generate only the answer, do not generate anything else\n",
    "\"\"\"\n",
    "\n",
    "        self.PROMPT_TEMPLATE = \"\"\"\n",
    "Now for the real task, solve the task in question block based on the context in context block.\n",
    "Generate only the solution, do not generate anything else.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\"\n",
    "\n",
    "    def retrieve(self, question: str, k: int = 3):\n",
    "        return self.vector_store.similarity_search(question, k=k)\n",
    "\n",
    "    def generate(self, question: str, context_docs):\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in context_docs)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": self.PROMPT_TEMPLATE.format(context=context, question=question)},\n",
    "        ]\n",
    "        return self.llm.invoke(messages).content\n",
    "\n",
    "    def answer(self, question: str):\n",
    "        return self.generate(question, self.retrieve(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca350f-909a-45a3-89b5-da3e75488d9d",
   "metadata": {},
   "source": [
    "### Plug the new model into RAG\n",
    "With the fine‑tuned weights now running locally, the last step is to introduce the specialized LLM into our existing RAG helper class. The retrieval component fetches the most relevant passages about the Roman Empire—while the generation step leverages a model that has been trained on our domain‑specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87c130-d6d6-41f8-a1a9-1a7a019a5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tuned_llm = ChatOpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key=\"EMPTY\",\n",
    "    model=\"model-distillabs\"\n",
    ")\n",
    "tuned_rag = RAG(vector_store=vector_store, llm=tuned_llm)\n",
    "print(tuned_rag.answer(\"When did the roman empire collapse?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33653e93-e07a-4852-8fb1-2451ca6b045c",
   "metadata": {},
   "source": [
    "### Test our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d75b0-5ffb-4393-82eb-0852ebb8b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "results = []\n",
    "for idx, example in test_dataset.iterrows():\n",
    "    reference = example[\"answer\"]\n",
    "    prediction = tuned_rag.answer(example[\"question\"])\n",
    "    metric = rouge.score(\n",
    "        target=reference,\n",
    "        prediction=prediction,\n",
    "    )\n",
    "    results.append(metric[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(f\"Aggregate metric: {np.mean(results)} +/- {np.std(results)}\")\n",
    "# Aggregate metric: 0.253503946795671 +/- 0.16689695917279943"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
